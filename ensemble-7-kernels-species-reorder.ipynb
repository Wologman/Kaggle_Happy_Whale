{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cba482",
   "metadata": {
    "papermill": {
     "duration": 0.009704,
     "end_time": "2022-04-18T03:24:02.509272",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.499568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I'm trying to ensemble these 7 notebooks, along with using a species classifier to down-prioritize any results that are the wrong species, since the species classifier is more accurate than the individual results.\n",
    "\n",
    "The original ensemble notebook : https://www.kaggle.com/code/yamsam/simple-ensemble-of-public-best-kernels  \n",
    "\n",
    "I'm using a species classifier on the test dataset from this FastAI kernel https://www.kaggle.com/code/kwentar/species-classification/notebook.  This idea didn't help.  It seems that between 7 classifiers, it seems species deliniation is actually pretty good.  A better approach might have been to use the classifier to build seperate models for each species.  Or maybe just Dolphin/Whale/Beluga Whale since those all had quite different distinguishing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd1569",
   "metadata": {
    "papermill": {
     "duration": 0.012765,
     "end_time": "2022-04-18T03:24:02.535919",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.523154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8282e48",
   "metadata": {
    "papermill": {
     "duration": 0.012601,
     "end_time": "2022-04-18T03:24:02.561780",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.549179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ensembelling the following kernels:\n",
    "\n",
    "* https://www.kaggle.com/code/nghiahoangtrung/swin-tranform-submission  (0.764 SWIN Transform, backfin TFrecords)\n",
    "\n",
    "*  https://www.kaggle.com/aikhmelnytskyy/happywhale-arcface-baseline-eff7-tpu-768-inference  (0.729 ArcFace Eff7 768 5 Folds)\n",
    "\n",
    "* https://www.kaggle.com/code/clemchris/pytorch-backfin-convnext-arcface  (0.725 Best82. Output CSV dataset Only)\n",
    "\n",
    "* https://www.kaggle.com/nghiahoangtrung/0-720-eff-b5-640-rotate (0.72 Eff B5 With Rotation, this one is already using a cropped Backfin dataset)\n",
    "\n",
    "* https://www.kaggle.com/aikhmelnytskyy/happywhale-effnet-b7-fork-with-detic-training  (0.699 ArcFace Effnet B7 + Detic Cropping)\n",
    "\n",
    "* https://www.kaggle.com/code/ollypowell/fastai-baseline-model (0.682 FastAI ArcFace, Backfin TFrecords)\n",
    "\n",
    "* https://www.kaggle.com/code/librauee/arcfaceeffb6inferbaseline  (0.655 Effnet B6, I should try and improve on this with SOD, object detect, horizontal flip + rotate, more folds)\n",
    "\n",
    "\n",
    "The dataset for the species classification provided as a .csv seperately here: https://www.kaggle.com/datasets/kwentar/happywhale-test-species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3286273f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:02.597550Z",
     "iopub.status.busy": "2022-04-18T03:24:02.596625Z",
     "iopub.status.idle": "2022-04-18T03:24:02.607378Z",
     "shell.execute_reply": "2022-04-18T03:24:02.608073Z",
     "shell.execute_reply.started": "2022-04-17T10:15:59.179058Z"
    },
    "papermill": {
     "duration": 0.033734,
     "end_time": "2022-04-18T03:24:02.608381",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.574647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The submission weightings:  [0.6677901945970756, 0.5314409999999999, 0.44755356042950317, 0.37324799999999997, 0.2855420775971462, 0.21634033537600006, 0.1489655519011116]\n"
     ]
    }
   ],
   "source": [
    "# List the submission paths and the weightings, using public leaderboard scores\n",
    "\n",
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "sub_files = [    '../input/swin-tranform-submission/submission.csv',  # LB 0.764 I added this, it made a big improvement\n",
    "                 '../input/happywhale-arcface-baseline-eff7-tpu-768-inference/submission.csv',  # LB 0.729 Not much I can do to imrove this one in time available\n",
    "                 '../input/best82/submission.csv',  # LB 0.725  Not much I can do to imrove this one in time available\n",
    "                 '../input/0-720-eff-b5-640-rotate/submission.csv', # LB 0.72\n",
    "                 '../input/happywhale-effnet-b7-fork-with-detic-crop/submission.csv', #  0.699 \n",
    "                 '../input/fastai-baseline-model/submission.csv',  #0.682\n",
    "                 '../input/arcfaceeffb6inferbaseline/submission.csv' # 0.655  Should try with improved dataset & augmentation on colab              \n",
    "]\n",
    "\n",
    "# Weights of the individual subs    I'm using this fomula, it is only fitted to the leaderboard in the sense that it uses the public scores, as biases\n",
    "# Another version of this notebook looked like somebody had hand tweaked the biases to fit the public leaderboard, I doubt that would generalise well.\n",
    "\n",
    "N_wight=0.5\n",
    "Base_wight=1.5\n",
    "SPECIES_PENALTY = 1   # The penalty if the id predictions do not match the expected species from the classifier\n",
    "\n",
    "l1 = [0.764, 0.729, 0.725, 0.72, 0.699, 0.682, 0.655]    # Relative weightings of the datasets\n",
    "l2 = [Base_wight+N_wight*i for i in range(len(l1))]    # Formula\n",
    "sub_weight = list(map(lambda x,y: x**y ,l1,l2))\n",
    "print('The submission weightings: ', sub_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74d47ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:02.639955Z",
     "iopub.status.busy": "2022-04-18T03:24:02.639108Z",
     "iopub.status.idle": "2022-04-18T03:24:02.722407Z",
     "shell.execute_reply": "2022-04-18T03:24:02.721682Z",
     "shell.execute_reply.started": "2022-04-17T10:15:59.302818Z"
    },
    "papermill": {
     "duration": 0.100192,
     "end_time": "2022-04-18T03:24:02.722596",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.622404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                image  label         species\n",
      "0  000110707af0ba.jpg      8      gray_whale\n",
      "1  0006287ec424cb.jpg      1  humpback_whale\n",
      "2  000809ecb2ccad.jpg      4          beluga\n",
      "3  00098d1376dab2.jpg      1  humpback_whale\n",
      "4  000b8d89c738bd.jpg     13   dusky_dolphin\n",
      "\n",
      " Dictionary key-value tuples:  [('000110707af0ba.jpg', 'gray_whale'), ('0006287ec424cb.jpg', 'humpback_whale'), ('000809ecb2ccad.jpg', 'beluga'), ('00098d1376dab2.jpg', 'humpback_whale'), ('000b8d89c738bd.jpg', 'dusky_dolphin')]\n"
     ]
    }
   ],
   "source": [
    "#Read the species classification file and make a image file - species dictionary\n",
    "\n",
    "df = pd.read_csv (r'../input/happywhale-test-species/Happywhale_test_species.csv')\n",
    "print(df.head())\n",
    "\n",
    "species_dict = dict(zip(df.image, df.species))\n",
    "\n",
    "print('\\n Dictionary key-value tuples: ', list(species_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c2a01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:02.759360Z",
     "iopub.status.busy": "2022-04-18T03:24:02.758548Z",
     "iopub.status.idle": "2022-04-18T03:24:02.904288Z",
     "shell.execute_reply": "2022-04-18T03:24:02.903470Z",
     "shell.execute_reply.started": "2022-04-17T10:15:59.415142Z"
    },
    "papermill": {
     "duration": 0.16709,
     "end_time": "2022-04-18T03:24:02.904474",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.737384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                image             species individual_id\n",
      "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9\n",
      "1  000562241d384d.jpg      humpback_whale  1a71fbb72250\n",
      "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b\n",
      "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063\n",
      "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['beluga',\n",
       "  'blue_whale',\n",
       "  'bottlenose_dolphin',\n",
       "  'brydes_whale',\n",
       "  'commersons_dolphin',\n",
       "  'common_dolphin',\n",
       "  'cuviers_beaked_whale',\n",
       "  'dusky_dolphin',\n",
       "  'false_killer_whale',\n",
       "  'fin_whale',\n",
       "  'frasiers_dolphin',\n",
       "  'gray_whale',\n",
       "  'humpback_whale',\n",
       "  'killer_whale',\n",
       "  'long_finned_pilot_whale',\n",
       "  'melon_headed_whale',\n",
       "  'minke_whale',\n",
       "  'pantropic_spotted_dolphin',\n",
       "  'pygmy_killer_whale',\n",
       "  'rough_toothed_dolphin',\n",
       "  'sei_whale',\n",
       "  'short_finned_pilot_whale',\n",
       "  'southern_right_whale',\n",
       "  'spinner_dolphin',\n",
       "  'spotted_dolphin',\n",
       "  'white_sided_dolphin'],\n",
       " 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the CSV training file and correct for some misspelt entries\n",
    "\n",
    "train_df = pd.read_csv(r'../input/happy-whale-and-dolphin/train.csv')\n",
    "train_df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n",
    "                          \"pilot_whale\": \"short_finned_pilot_whale\",\n",
    "                          \"kiler_whale\": \"killer_whale\",\n",
    "                          \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)\n",
    "print(train_df.head())\n",
    "\n",
    "sorted(train_df.species.unique()), len(train_df.species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6956ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:02.949090Z",
     "iopub.status.busy": "2022-04-18T03:24:02.948109Z",
     "iopub.status.idle": "2022-04-18T03:24:02.984871Z",
     "shell.execute_reply": "2022-04-18T03:24:02.985477Z",
     "shell.execute_reply.started": "2022-04-17T10:15:59.572402Z"
    },
    "papermill": {
     "duration": 0.065654,
     "end_time": "2022-04-18T03:24:02.985703",
     "exception": false,
     "start_time": "2022-04-18T03:24:02.920049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  15587 unique individuals\n",
      "There are  15587 entries in id_species dictionary\n"
     ]
    }
   ],
   "source": [
    "#Use the training csv file to create an individual-species dictionary\n",
    "\n",
    "print ('There are ', len(train_df.individual_id.unique()), 'unique individuals')\n",
    "\n",
    "dict1 =  dict(zip(train_df.individual_id, train_df.species))     # Will contain duplicates  \n",
    "temp = {val: key for val, key in dict1.items()}\n",
    "id_species = {val: key for val, key in temp.items()}\n",
    "\n",
    "print ('There are ', len(id_species.items()), 'entries in id_species dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb1214f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:03.028202Z",
     "iopub.status.busy": "2022-04-18T03:24:03.027375Z",
     "iopub.status.idle": "2022-04-18T03:24:04.946807Z",
     "shell.execute_reply": "2022-04-18T03:24:04.946180Z",
     "shell.execute_reply.started": "2022-04-17T10:15:59.633893Z"
    },
    "papermill": {
     "duration": 1.945285,
     "end_time": "2022-04-18T03:24:04.946955",
     "exception": false,
     "start_time": "2022-04-18T03:24:03.001670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 0.5, 2: 0.3333333333333333, 3: 0.25, 4: 0.2, 5: 0.16666666666666666}\n",
      "Reading 0: w=0.6677901945970756 - ../input/swin-tranform-submission/submission.csv\n",
      "Reading 1: w=0.5314409999999999 - ../input/happywhale-arcface-baseline-eff7-tpu-768-inference/submission.csv\n",
      "Reading 2: w=0.44755356042950317 - ../input/best82/submission.csv\n",
      "Reading 3: w=0.37324799999999997 - ../input/0-720-eff-b5-640-rotate/submission.csv\n",
      "Reading 4: w=0.2855420775971462 - ../input/happywhale-effnet-b7-fork-with-detic-crop/submission.csv\n",
      "Reading 5: w=0.21634033537600006 - ../input/fastai-baseline-model/submission.csv\n",
      "Reading 6: w=0.1489655519011116 - ../input/arcfaceeffb6inferbaseline/submission.csv\n",
      "7\n",
      "27956\n",
      "[OrderedDict([('image', '000110707af0ba.jpg'), ('predictions', 'fbe2b15b5481 new_individual 54a19a45715c a68efd897f36 74062c624dea')]), OrderedDict([('image', '0006287ec424cb.jpg'), ('predictions', '1424c7fec826 new_individual c3b7d902e73c 08e7aae3e88e 98995caaa0d3')]), OrderedDict([('image', '000809ecb2ccad.jpg'), ('predictions', '1ce3ba6a3c29 new_individual 20c4589b5b16 549658000b20 9b9d0e198977')])]\n"
     ]
    }
   ],
   "source": [
    "#Read and order the CSV output files from each submission\n",
    "\n",
    "Hlabel = 'image' \n",
    "Htarget = 'predictions'\n",
    "npt = 6\n",
    "place_weights = {}\n",
    "for i in range(npt):\n",
    "    place_weights[i] = (1 / (i + 1))   # Makes this: {0: 1.0, 1: 0.5, 2: 0.3333333333333333, 3: 0.25, 4: 0.2, 5: 0.16666666666666666}\n",
    "\n",
    "print(place_weights)\n",
    "\n",
    "lg = len(sub_files)    # Number of files to ensemble\n",
    "sub = [None]*lg        # Empty String with that number of values = None\n",
    "for i, file in enumerate( sub_files ):          #Loop through the filepath strings, incrementing i from 0 to one less than number of files\n",
    "    print(\"Reading {}: w={} - {}\". format(i, sub_weight[i], file))    \n",
    "    reader = csv.DictReader(open(file,\"r\"))               #Open a file with DictReader\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel])) #sorted dictionary by the Hlabel key (makes sure 'image' first, then 'predictions')\n",
    "\n",
    "print(len(sub)) #=6 \n",
    "print(len(sub[0])) #= 27956\n",
    "print(sub[0][:3]) # sub is a list of ordered dictionaries  \n",
    "#OrderedDict([('image', '000110707af0ba.jpg'), ('predictions', 'fbe2b15b5481 new_individual 54a19a45715c a68efd897f36 74062c624dea')])   x 27956, x 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daee837",
   "metadata": {
    "papermill": {
     "duration": 0.018625,
     "end_time": "2022-04-18T03:24:04.985306",
     "exception": false,
     "start_time": "2022-04-18T03:24:04.966681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Write the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc70b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:24:05.027535Z",
     "iopub.status.busy": "2022-04-18T03:24:05.026784Z",
     "iopub.status.idle": "2022-04-18T03:24:07.471071Z",
     "shell.execute_reply": "2022-04-18T03:24:07.471678Z",
     "shell.execute_reply.started": "2022-04-17T10:16:01.021004Z"
    },
    "papermill": {
     "duration": 2.467444,
     "end_time": "2022-04-18T03:24:07.471921",
     "exception": false,
     "start_time": "2022-04-18T03:24:05.004477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'32a8f92d7809': 1.7049320904025789, 'new_individual': 1.335440359950418, '64bb5df01cdf': 0.506507331018825, '556d610a3896': 0.2732357486492689, 'f187914208a4': 0.18764312276341513, 'ad4557cf336c': 0.6749399645246199, '4587a8786179': 0.41840232759714624, 'a7a5696645cb': 0.14918452014316771, 'f7ce926d1f9a': 0.11188839010737579, '741f7de87a7b': 0.08951071208590064, '783f357cf2f9': 0.2733815519011116, '18dbd44823c7': 0.09331199999999999, 'ad981a124bbd': 0.0746496, '44a2f97c19c3': 0.09518069253238207, '7ef48fb30101': 0.04326806707520001, '5b7ef1ff1875': 0.0372413879752779, 'e59ada492aeb': 0.029793110380222323}\n"
     ]
    }
   ],
   "source": [
    "out = open(\"submission.csv\", \"w\", newline='')\n",
    "writer = csv.writer(out)\n",
    "writer.writerow([Hlabel,Htarget])     #This is writing the header row  \"image\", \"predictions\"\n",
    "\n",
    "wrong_species = 0 \n",
    "correct_species = 0\n",
    "new_individual = 0\n",
    "#changed = 0\n",
    "\n",
    "for p, row in enumerate(sub[0]):     #iterate the rows of each image\n",
    "    target_weight = {}\n",
    "    for s in range(lg):              #iterate results of each kernel in sub   \n",
    "        row1 = sub[s][p]             #row1 is from the sth kernel, and the pth ordered dic.  \n",
    "        #example: [('image', '000110707af0ba.jpg'), ('predictions', 'fbe2b15b5481 new_individual 54a19a45715c a68efd897f36 74062c624dea')])\n",
    "        image_file = row1[Hlabel]\n",
    "        for ind, trgt in enumerate(row1[Htarget].split(' ')):  #gets each prediction   ind is the position, trgt is the id\n",
    "            # a dictionary with each target, adding the sum from previous kernels estimates + place_weight*sub_weight \n",
    "            #target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s]) \n",
    "            #So I'm modifying this to penalise predictions that are not the expected species\n",
    "            image_species = species_dict[image_file]   # produces a string corresponding to the species\n",
    "            if trgt == 'new_individual':\n",
    "                species_weight = 1\n",
    "                new_individual += 1        \n",
    "            else: \n",
    "                if id_species[trgt] == image_species:     #Both Strings.\n",
    "                    species_weight = 1   \n",
    "                    correct_species += 1\n",
    "                else: \n",
    "                    species_weight = SPECIES_PENALTY  # Wrong species penalty\n",
    "                    wrong_species +=1\n",
    "            \n",
    "            target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s]*species_weight) \n",
    "    \n",
    "    #if max(list(target_weight.values())) < 0.01:  # The case when they are all 'wrong species'  It would be better to guess new individual\n",
    "        #tops_trgt = ['new_individual'] + sorted(target_weight, key=target_weight.get, reverse=True)[:npt-1]\n",
    "        #changed +=1\n",
    "    #else:            \n",
    "    tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]   # a list of targets sorted by weight  1 extra so I can add a new individual\n",
    "    \n",
    "    writer.writerow([row1[Hlabel], \" \".join(tops_trgt)])\n",
    "print(target_weight)\n",
    "out.close()\n",
    "\n",
    "#print(wrong_species, correct_species, new_individual, changed)  # This didn't help.  The kernels seem to be doing a pretty good job of getting the species right!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.603019,
   "end_time": "2022-04-18T03:24:08.704827",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-18T03:23:50.101808",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
